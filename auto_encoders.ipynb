{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a type of neural network used to learn efficient codings of unlabeled data. It is designed for unsupervised machine learning. An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations of Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations.\n",
    "\n",
    "- **Sparse Autoencoders:** Sparse autoencoders introduce a sparsity constraint on the latent space representation.\n",
    "This encourages the model to learn a compact and sparse representation of the data, which can be useful for feature extraction and dimensionality reduction. Sparse autoencoders are often used when the input data has many irrelevant features, helping the model to focus on the most important ones.\n",
    "\n",
    "- **Denoising Autoencoders:** In this type of autoencoder, noise is added to the input data, and the network is trained to reconstruct the original, noise-free data. By learning to remove noise from the input, denoising autoencoders can capture robust features of the data. They are useful for tasks where data may be corrupted or noisy.\n",
    "\n",
    "- **Contractive Autoencoders:** A CAE compresses input data into a lower-dimensional representation and then reconstructs the original data. It includes a regularization term that penalizes changes in the encoder's output with respect to small changes in the input data, encouraging the model to learn stable and invariant representations. CAEs are useful for tasks like data denoising, anomaly detection, and feature learning, providing compact and robust representations while maintaining the ability to reconstruct the input data.\n",
    "\n",
    "- **Variational Autoencoders:** VAEs are probabilistic autoencoders that learn a latent variable model for the data.\n",
    "Instead of encoding data into a single fixed point in the latent space, VAEs map data to a probability distribution in the latent space. This allows for generating new data samples by sampling from the learned distribution. VAEs are commonly used for generating new data samples, such as in image generation tasks.\n",
    "\n",
    "- **Stacked Autoencoders:** Stacked autoencoders consist of multiple layers of autoencoders stacked sequentially. Each layer encodes the input data into a more abstract representation, which serves as the input for the next layer. They are trained layer by layer using unsupervised learning techniques and are commonly used for feature learning and dimensionality reduction tasks.\n",
    "\n",
    "- **Deep Autoencoders:** A deep autoencoder consists of multiple hidden layers in both the encoder and decoder parts of the network. These additional layers allow for learning more complex and hierarchical representations of the input data. Deep autoencoders are capable of capturing intricate features and structures in the data, making them suitable for tasks like image denoising, feature learning, and data compression. Deep Autoencoders are not the same as Stacked Autoencoders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
