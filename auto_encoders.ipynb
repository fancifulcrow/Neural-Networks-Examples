{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a type of neural network used to learn efficient codings of unlabeled data. It is designed for unsupervised machine learning. An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\n",
    "\n",
    "In this example, we are making a Movie Recommedation System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations of Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations.\n",
    "\n",
    "- **Sparse Autoencoders:** Sparse autoencoders introduce a sparsity constraint on the latent space representation.\n",
    "This encourages the model to learn a compact and sparse representation of the data, which can be useful for feature extraction and dimensionality reduction. Sparse autoencoders are often used when the input data has many irrelevant features, helping the model to focus on the most important ones.\n",
    "\n",
    "- **Denoising Autoencoders:** In this type of autoencoder, noise is added to the input data, and the network is trained to reconstruct the original, noise-free data. By learning to remove noise from the input, denoising autoencoders can capture robust features of the data. They are useful for tasks where data may be corrupted or noisy.\n",
    "\n",
    "- **Contractive Autoencoders:** A CAE compresses input data into a lower-dimensional representation and then reconstructs the original data. It includes a regularization term that penalizes changes in the encoder's output with respect to small changes in the input data, encouraging the model to learn stable and invariant representations. CAEs are useful for tasks like data denoising, anomaly detection, and feature learning, providing compact and robust representations while maintaining the ability to reconstruct the input data.\n",
    "\n",
    "- **Variational Autoencoders:** VAEs are probabilistic autoencoders that learn a latent variable model for the data.\n",
    "Instead of encoding data into a single fixed point in the latent space, VAEs map data to a probability distribution in the latent space. This allows for generating new data samples by sampling from the learned distribution. VAEs are commonly used for generating new data samples, such as in image generation tasks.\n",
    "\n",
    "- **Stacked Autoencoders:** Stacked autoencoders consist of multiple layers of autoencoders stacked sequentially. Each layer encodes the input data into a more abstract representation, which serves as the input for the next layer. They are trained layer by layer using unsupervised learning techniques and are commonly used for feature learning and dimensionality reduction tasks.\n",
    "\n",
    "- **Deep Autoencoders:** A deep autoencoder consists of multiple hidden layers in both the encoder and decoder parts of the network. These additional layers allow for learning more complex and hierarchical representations of the input data. Deep autoencoders are capable of capturing intricate features and structures in the data, making them suitable for tasks like image denoising, feature learning, and data compression. Deep Autoencoders are not the same as Stacked Autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is from [Kaggle](https://www.kaggle.com/datasets/akkefa/movielens-9000-movies-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not using the movies dataframe for training. We are just using it to know what is going on with all the movies. The id the most important column because this is what we will use to identify the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv(\"./data/movies.csv\")\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings are what we are using to train the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"./data/ratings.csv\")\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just going to do some adjustments and change the moviesId numbering for the movies and rating dataframe since the numbering is slightly messed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'id' to movies starting from 1\n",
    "movies['id'] = movies.reset_index().index + 1\n",
    "\n",
    "# Merge ratings with the movieId and id mapping\n",
    "ratings = ratings.merge(movies[[\"movieId\", \"id\"]], on=\"movieId\")\n",
    "\n",
    "# Replace the moviesId and remove id\n",
    "movies[\"movieId\"] = movies[\"id\"]\n",
    "ratings[\"movieId\"] = ratings[\"id\"]\n",
    "\n",
    "del movies[\"id\"]\n",
    "del ratings[\"id\"]\n",
    "\n",
    "# remove timestamp column from ratings dataframe as it is not used.\n",
    "del ratings[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        1     4.0\n",
       "1       1        3     4.0\n",
       "2       1        6     4.0\n",
       "3       1       44     5.0\n",
       "4       1       47     5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into training and test sets. We will be converting the training and test sets to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(ratings, test_size=0.2, random_state=37)\n",
    "\n",
    "train_set = np.array(train_set, dtype=\"int\")\n",
    "test_set = np.array(test_set, dtype=\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Total Number of Users and Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the total number of users and movies. This is because we are going to convert the training and test sets into matrices where the rows are the users, the columns represent the movies and each cell is the rating of a movie by a user.\n",
    "\n",
    "If a user did not rate a movie then the cell would be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The users and movies are both using sequential ids. So the total number would just be the last id.\n",
    "nb_users = max(ratings[\"userId\"])\n",
    "nb_movies = max(ratings[\"movieId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9742"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Data into a 2D Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a function that would convert our training and test sets into 2D matrices. Simply put, we want to create a list of lists. Each list would be the ratings of every movie by a particular user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data):\n",
    "    # Initializing a matrix with zeros to store the ratings\n",
    "    new_data = np.zeros((nb_users, nb_movies))\n",
    "    \n",
    "    # Iterating through each rating in the dataset\n",
    "    for i in range(len(data[:, 0])):\n",
    "        # Extracting user, movie, and rating from the current row\n",
    "        user = data[i, 0]\n",
    "        book = data[i, 1]\n",
    "        rating = data[i, 2]\n",
    "        \n",
    "        # Storing the rating in the appropriate position in the matrix\n",
    "        new_data[user - 1, book - 1] = rating\n",
    "\n",
    "    # Return the converted data matrix\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = convert_data(train_set)\n",
    "test_set = convert_data(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [2., 2., 2., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.],\n",
       "       [5., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Pytorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our training and test set to **pytorch tensors**.\n",
    "\n",
    "PyTorch tensors are multi-dimensional arrays similar to NumPy arrays, but with additional features and optimizations for deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.from_numpy(train_set).float()\n",
    "test_set = torch.from_numpy(test_set).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create an Autoencoder so the class we are going to create will contain instructions on how to build the autoencoder. To make the autoencoder using pytorch, we need to define multiple things: the layers, the number of layers, how many nodes in the layers, the activation function, the criterion, the optimizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a stacked autoencoder\n",
    "class SAE(nn.Module):\n",
    "    # Initializes the architecture of the stacked autoencoder.\n",
    "    # Defines the structure of the encoder and decoder layers, including fully connected hidden layers.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # These are the Fully Connected Hidden Layers\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(nb_movies, 20) # Fully connected layer from input (number of movies) to a hidden layer with 20 neurons\n",
    "        self.fc2 = nn.Linear(20, 10) # Fully connected layer from the first hidden layer (20 neurons) to a second hidden layer with 10 neurons\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(10, 20) # Fully connected layer from the second hidden layer (10 neurons) to a third hidden layer with 20 neurons\n",
    "        self.fc4 = nn.Linear(20, nb_movies) # Fully connected layer from the third hidden layer (20 neurons) to the output (number of movies)\n",
    "\n",
    "        # The activation function\n",
    "        self.activation = nn.Sigmoid() # Sigmoid activation function is used for non-linearity\n",
    "        \n",
    "    \n",
    "    # This method defines the forward pass computation of the stacked autoencoder.\n",
    "    # It describes how input data flows through the network's layers to produce a reconstructed output.\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x = self.activation(self.fc1(x)) # Pass input through the first fully connected layer and apply sigmoid activation\n",
    "        x = self.activation(self.fc2(x)) # Pass the result through the second fully connected layer and apply sigmoid activation\n",
    "        # Decoding\n",
    "        x = self.activation(self.fc3(x)) # Pass the result through the third fully connected layer and apply sigmoid activation\n",
    "        x = self.fc4(x) # Pass the result through the fourth fully connected layer without any activation\n",
    "        return x  # Return the reconstructed output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sets up the stacked autoencoder model, `sae`, defines the loss function, `criterion` to compute the reconstruction error, and initializes the RMSprop optimizer, `optimizer` with specific parameters for training the autoencoder model as follows:\n",
    "\n",
    "- `sae.parameters()` provides the parameters (weights and biases) of the autoencoder model to the optimizer for optimization.\n",
    "- `lr=0.01` sets the learning rate to 0.01, which controls the step size during optimization.\n",
    "- `weight_decay=0.5` applies L2 regularization to the optimizer with a regularization strength of 0.5. L2 regularization helps prevent overfitting by penalizing large weights in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a stacked autoencoder model using the provided training data. It iterates over each epoch and each user in the training set, prepares the input data, computes the reconstruction loss, and updates the model parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\tloss: 3.3699132188881458\n",
      "epoch: 2\tloss: 2.5347151419233906\n",
      "epoch: 3\tloss: 2.094292741595523\n",
      "epoch: 4\tloss: 1.8541444945824823\n",
      "epoch: 5\tloss: 1.6997055287471245\n",
      "epoch: 6\tloss: 1.5912896643315348\n",
      "epoch: 7\tloss: 1.5106088953023271\n",
      "epoch: 8\tloss: 1.4479673065772705\n",
      "epoch: 9\tloss: 1.3976987831449645\n",
      "epoch: 10\tloss: 1.356310089634932\n",
      "epoch: 11\tloss: 1.3215266989020547\n",
      "epoch: 12\tloss: 1.2918052236087898\n",
      "epoch: 13\tloss: 1.2660537021577922\n",
      "epoch: 14\tloss: 1.2434845354511628\n",
      "epoch: 15\tloss: 1.223509546980243\n",
      "epoch: 16\tloss: 1.2056854334781908\n",
      "epoch: 17\tloss: 1.1896660055948423\n",
      "epoch: 18\tloss: 1.1751795192293204\n",
      "epoch: 19\tloss: 1.1620045527181753\n",
      "epoch: 20\tloss: 1.1499661283955416\n",
      "epoch: 21\tloss: 1.1389170300835056\n",
      "epoch: 22\tloss: 1.1287369128796663\n",
      "epoch: 23\tloss: 1.1193209077235273\n",
      "epoch: 24\tloss: 1.110585733436947\n",
      "epoch: 25\tloss: 1.1024570129507913\n",
      "epoch: 26\tloss: 1.094873005686984\n",
      "epoch: 27\tloss: 1.0877755893037224\n",
      "epoch: 28\tloss: 1.0811210114639962\n",
      "epoch: 29\tloss: 1.0748668526086322\n",
      "epoch: 30\tloss: 1.0689784048545241\n",
      "epoch: 31\tloss: 1.063419614111155\n",
      "epoch: 32\tloss: 1.0581662177619322\n",
      "epoch: 33\tloss: 1.0531919710601914\n",
      "epoch: 34\tloss: 1.0484761616825031\n",
      "epoch: 35\tloss: 1.043994324210855\n",
      "epoch: 36\tloss: 1.0397327814565491\n",
      "epoch: 37\tloss: 1.0356740893659255\n",
      "epoch: 38\tloss: 1.0318055238776038\n",
      "epoch: 39\tloss: 1.0281089736543414\n",
      "epoch: 40\tloss: 1.024577262480056\n",
      "epoch: 41\tloss: 1.021198108971085\n",
      "epoch: 42\tloss: 1.0179636942589925\n",
      "epoch: 43\tloss: 1.0148585901799965\n",
      "epoch: 44\tloss: 1.0118812480007227\n",
      "epoch: 45\tloss: 1.0090216508854366\n",
      "epoch: 46\tloss: 1.0062748560890764\n",
      "epoch: 47\tloss: 1.0036289330769972\n",
      "epoch: 48\tloss: 1.0010833190179897\n",
      "epoch: 49\tloss: 0.9986308124410627\n",
      "epoch: 50\tloss: 0.996268707929209\n",
      "epoch: 51\tloss: 0.9939862371499744\n",
      "epoch: 52\tloss: 0.9917846503228993\n",
      "epoch: 53\tloss: 0.9896580498946337\n",
      "epoch: 54\tloss: 0.987605097128438\n",
      "epoch: 55\tloss: 0.9856161670213139\n",
      "epoch: 56\tloss: 0.9836939286098455\n",
      "epoch: 57\tloss: 0.9818328716057535\n",
      "epoch: 58\tloss: 0.9800327917379376\n",
      "epoch: 59\tloss: 0.9782847538519037\n",
      "epoch: 60\tloss: 0.9765924477347043\n",
      "epoch: 61\tloss: 0.9749510572581743\n",
      "epoch: 62\tloss: 0.9733612012574057\n",
      "epoch: 63\tloss: 0.9718154677597025\n",
      "epoch: 64\tloss: 0.97031250354099\n",
      "epoch: 65\tloss: 0.9688557267953719\n",
      "epoch: 66\tloss: 0.9674422682074856\n",
      "epoch: 67\tloss: 0.9660645519765599\n",
      "epoch: 68\tloss: 0.9647270991813485\n",
      "epoch: 69\tloss: 0.963425871492338\n",
      "epoch: 70\tloss: 0.962162385108248\n",
      "epoch: 71\tloss: 0.9609287487129191\n",
      "epoch: 72\tloss: 0.9597300469073735\n",
      "epoch: 73\tloss: 0.9585624908472042\n",
      "epoch: 74\tloss: 0.9574278334601947\n",
      "epoch: 75\tloss: 0.9563183434290835\n",
      "epoch: 76\tloss: 0.9552393334048184\n",
      "epoch: 77\tloss: 0.954187242824385\n",
      "epoch: 78\tloss: 0.9531641136043696\n",
      "epoch: 79\tloss: 0.9521623949295897\n",
      "epoch: 80\tloss: 0.951187976228627\n",
      "epoch: 81\tloss: 0.9502343959204241\n",
      "epoch: 82\tloss: 0.949309265268133\n",
      "epoch: 83\tloss: 0.9484015376042482\n",
      "epoch: 84\tloss: 0.9475175886759754\n",
      "epoch: 85\tloss: 0.9466540542512412\n",
      "epoch: 86\tloss: 0.9458132926201098\n",
      "epoch: 87\tloss: 0.9449879421766435\n",
      "epoch: 88\tloss: 0.9441838256264491\n",
      "epoch: 89\tloss: 0.9433978622241632\n",
      "epoch: 90\tloss: 0.9426314080526751\n",
      "epoch: 91\tloss: 0.9418789184822177\n",
      "epoch: 92\tloss: 0.9411522146449868\n",
      "epoch: 93\tloss: 0.9404323289156703\n",
      "epoch: 94\tloss: 0.9397330033290177\n",
      "epoch: 95\tloss: 0.9390455982967826\n",
      "epoch: 96\tloss: 0.9383754731806424\n",
      "epoch: 97\tloss: 0.9377196081418909\n",
      "epoch: 98\tloss: 0.9370805713256548\n",
      "epoch: 99\tloss: 0.936451165539557\n",
      "epoch: 100\tloss: 0.9358375217366192\n",
      "epoch: 101\tloss: 0.9352365628713772\n",
      "epoch: 102\tloss: 0.934650948447016\n",
      "epoch: 103\tloss: 0.9340735341560902\n",
      "epoch: 104\tloss: 0.9335104841131132\n",
      "epoch: 105\tloss: 0.9329588008101037\n",
      "epoch: 106\tloss: 0.932421167670512\n",
      "epoch: 107\tloss: 0.9318904635527419\n",
      "epoch: 108\tloss: 0.9313729460881045\n",
      "epoch: 109\tloss: 0.9308656020990899\n",
      "epoch: 110\tloss: 0.9303712166157911\n",
      "epoch: 111\tloss: 0.929882695666748\n",
      "epoch: 112\tloss: 0.9294062929819973\n",
      "epoch: 113\tloss: 0.928939064940015\n",
      "epoch: 114\tloss: 0.9284838310505155\n",
      "epoch: 115\tloss: 0.9280335495956669\n",
      "epoch: 116\tloss: 0.9275943476872778\n",
      "epoch: 117\tloss: 0.9271635746644532\n",
      "epoch: 118\tloss: 0.9267438734672806\n",
      "epoch: 119\tloss: 0.9263282684320978\n",
      "epoch: 120\tloss: 0.9259230655333096\n",
      "epoch: 121\tloss: 0.9255253638958562\n",
      "epoch: 122\tloss: 0.9251380219362905\n",
      "epoch: 123\tloss: 0.9247539927055538\n",
      "epoch: 124\tloss: 0.9243796420656717\n",
      "epoch: 125\tloss: 0.9240123718727016\n",
      "epoch: 126\tloss: 0.9236545689978632\n",
      "epoch: 127\tloss: 0.9232994811743996\n",
      "epoch: 128\tloss: 0.9229531634428628\n",
      "epoch: 129\tloss: 0.9226131680558812\n",
      "epoch: 130\tloss: 0.9222824517499955\n",
      "epoch: 131\tloss: 0.9219537230741887\n",
      "epoch: 132\tloss: 0.9216334327282625\n",
      "epoch: 133\tloss: 0.9213187901099825\n",
      "epoch: 134\tloss: 0.9210126578964754\n",
      "epoch: 135\tloss: 0.9207081015503634\n",
      "epoch: 136\tloss: 0.9204114490532073\n",
      "epoch: 137\tloss: 0.920119966084033\n",
      "epoch: 138\tloss: 0.9198364739244643\n",
      "epoch: 139\tloss: 0.919554106313892\n",
      "epoch: 140\tloss: 0.9192791652438741\n",
      "epoch: 141\tloss: 0.9190089385011738\n",
      "epoch: 142\tloss: 0.9187462560131859\n",
      "epoch: 143\tloss: 0.9184842714974019\n",
      "epoch: 144\tloss: 0.9182292879476232\n",
      "epoch: 145\tloss: 0.9179786136266779\n",
      "epoch: 146\tloss: 0.9177350819121779\n",
      "epoch: 147\tloss: 0.9174918536156156\n",
      "epoch: 148\tloss: 0.9172552429304943\n",
      "epoch: 149\tloss: 0.9170225706566218\n",
      "epoch: 150\tloss: 0.9167966818306755\n",
      "epoch: 151\tloss: 0.9165707398220402\n",
      "epoch: 152\tloss: 0.916351071920677\n",
      "epoch: 153\tloss: 0.9161350099345577\n",
      "epoch: 154\tloss: 0.9159253871363688\n",
      "epoch: 155\tloss: 0.9157154033100594\n",
      "epoch: 156\tloss: 0.9155113675253308\n",
      "epoch: 157\tloss: 0.9153106268277816\n",
      "epoch: 158\tloss: 0.91511603226741\n",
      "epoch: 159\tloss: 0.9149207809641194\n",
      "epoch: 160\tloss: 0.9147311782925093\n",
      "epoch: 161\tloss: 0.9145446069598724\n",
      "epoch: 162\tloss: 0.9143638816887876\n",
      "epoch: 163\tloss: 0.914182254370531\n",
      "epoch: 164\tloss: 0.9140059983193778\n",
      "epoch: 165\tloss: 0.9138325214957163\n",
      "epoch: 166\tloss: 0.9136646493326339\n",
      "epoch: 167\tloss: 0.9134956152485036\n",
      "epoch: 168\tloss: 0.9133317180587069\n",
      "epoch: 169\tloss: 0.9131703644278527\n",
      "epoch: 170\tloss: 0.9130143836901009\n",
      "epoch: 171\tloss: 0.9128570270532899\n",
      "epoch: 172\tloss: 0.9127045676810216\n",
      "epoch: 173\tloss: 0.9125544505748291\n",
      "epoch: 174\tloss: 0.9124094780890448\n",
      "epoch: 175\tloss: 0.9122629308521882\n",
      "epoch: 176\tloss: 0.9121210758065137\n",
      "epoch: 177\tloss: 0.911981376883474\n",
      "epoch: 178\tloss: 0.9118466128029777\n",
      "epoch: 179\tloss: 0.911710095567388\n",
      "epoch: 180\tloss: 0.9115780696579074\n",
      "epoch: 181\tloss: 0.9114480258379284\n",
      "epoch: 182\tloss: 0.9113227345928895\n",
      "epoch: 183\tloss: 0.9111955188417009\n",
      "epoch: 184\tloss: 0.9110726194640978\n",
      "epoch: 185\tloss: 0.9109515332666211\n",
      "epoch: 186\tloss: 0.9108350316749327\n",
      "epoch: 187\tloss: 0.9107164474804835\n",
      "epoch: 188\tloss: 0.9106020230803646\n",
      "epoch: 189\tloss: 0.9104892611175223\n",
      "epoch: 190\tloss: 0.9103809312100415\n",
      "epoch: 191\tloss: 0.9102703755634988\n",
      "epoch: 192\tloss: 0.9101638316441564\n",
      "epoch: 193\tloss: 0.9100588398396676\n",
      "epoch: 194\tloss: 0.9099582086745638\n",
      "epoch: 195\tloss: 0.9098555380334621\n",
      "epoch: 196\tloss: 0.9097574772937603\n",
      "epoch: 197\tloss: 0.9096613159585285\n",
      "epoch: 198\tloss: 0.9095687194541958\n",
      "epoch: 199\tloss: 0.9094723788325833\n",
      "epoch: 200\tloss: 0.9093798456181295\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 200 # The total number of epoch we will run\n",
    "\n",
    "# Iterate over the specified number of epochs\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    # Initialize variables to track training loss and number of processed samples\n",
    "    train_loss = 0\n",
    "    s = 0.0\n",
    "\n",
    "    # Iterate over each user in the training set\n",
    "    for id_user in range(nb_users):\n",
    "        # Get the input data for the current user and add an extra dimension (unsqueeze) to match model input shape\n",
    "        model_input = train_set[id_user].unsqueeze(0)\n",
    "        # Clone the input data to use as target for computing loss\n",
    "        target = model_input.clone()\n",
    "\n",
    "        # Check if the target data contains nonzero values (i.e., if the user has rated any movies)\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            # Pass the input data through the autoencoder to get the reconstructed output\n",
    "            model_output = sae(model_input)\n",
    "            # Freeze the target data to prevent gradients from being computed for these values\n",
    "            target.requires_grad = False\n",
    "            # Mask the output where target values are zero (unrated movies)\n",
    "            model_output[target == 0] = 0\n",
    "\n",
    "            # Compute the loss between the model output and target\n",
    "            loss = criterion(model_output, target)\n",
    "            # Compute the mean corrector factor to adjust loss scale\n",
    "            # We added a very small number to the denominator to prevent a DividedByZero Error\n",
    "            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "            # Compute gradients and backpropagate the loss\n",
    "            loss.backward()\n",
    "            # Update the training loss by adding the scaled loss\n",
    "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "\n",
    "            # Increment the count of processed samples\n",
    "            s += 1.0\n",
    "\n",
    "            # Update model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "    # Print epoch number and average loss for the epoch\n",
    "    print(f\"epoch: {epoch}\\tloss: {train_loss / s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will test the our autoencoder using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.024999052430839\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0  # Initialize test loss\n",
    "s = 0.0  # Initialize count of processed samples\n",
    "\n",
    "# Iterate over each user in the test set\n",
    "for id_user in range(nb_users):\n",
    "    # Get the input data for the current user from the training set and add an extra dimension\n",
    "    model_input = train_set[id_user].unsqueeze(0)\n",
    "    # Get the target data for the current user from the test set and add an extra dimension\n",
    "    target = test_set[id_user].unsqueeze(0)\n",
    "\n",
    "    # Check if the target data contains nonzero values (i.e., if the user has rated any movies)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        # Pass the input data through the autoencoder to get the reconstructed output\n",
    "        model_output = sae(model_input)\n",
    "        # Freeze the target data to prevent gradients from being computed for these values\n",
    "        target.requires_grad = False\n",
    "        # Mask the output where target values are zero (unrated movies)\n",
    "        model_output[target == 0] = 0\n",
    "\n",
    "        # Compute the loss between the model output and target\n",
    "        loss = criterion(model_output, target)\n",
    "        # Compute the mean corrector factor to adjust loss scale\n",
    "        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "        # Update the test loss by adding the scaled loss\n",
    "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "\n",
    "        # Increment the count of processed samples\n",
    "        s += 1.0\n",
    "\n",
    "# Calculate and print the average test loss\n",
    "print(f\"test loss: {test_loss / s}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
