{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a Convolutional Neural Network that differentiates between cats and dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to perform some **transformations** (such as rotation, flip, zoom, translation, shear e.t.c) on the images of the training set. The idea behind these transformations is to expose the model to a diverse range of input variations during training, which helps prevent it from memorizing specific details of the training set. This way we can avoid **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rescale` parameter of the `ImageDataGenerator` will apply **feature scaling** to every pixel. By scaling the pixel values by 1/255, we normalize them to the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a generator that will read images found in subfolers of 'data/train', and indefinitely generate batches of augmented image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8005 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='binary') # this is a binary classification problem i.e either cats or dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to perform transformations on the test data to avoid **data leakage**. We want to ensure that the evaluation of the model's performance reflects its ability to generalize to new, unseen data in a real-world scenario. We do however still need to normalize the data for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary to the training, we create a generator for the test set. Only difference is the directory of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2023 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "        'data/test',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Convolutional Neural Network is a sequence of layers. Therefore we are going to intialize our layer with the `Sequential` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the Convolution Layer with the `Conv2D` class to our model while specifying `filters` (number of feature detectors you want to apply to your images), `kernel_size` (the size of the feature detectors), `activation` (the activation function) and `input_shape` (the input shape).\n",
    "\n",
    "When we add the very first layer, whether a convolution layer or a dense layer, we have to specify the input shape of our inputs. The input shape is (150, 150, 3) as the size of our images after preprocessing is 150x150 and we are using coloured images, 3 dimensions corresponding to the RGB channels.\n",
    "\n",
    "For other layers after the input layer, we do not specify the input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(150, 150, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add a layer for pooling. More specifically, **max pooling**. The `MaxPool2D` class has two necessary parameters: `pool_size` (size of the pooling window) and `strides` (step size of the pooling window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Second Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though, while not necessary, using a second convolutional layer in a Convolutional Neural Network allows the model to capture higher-level features by learning more complex patterns and representations from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow up by flattening the results of our convolutions and poolings into a one-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass it on to a fully connected Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join up to fully connected layers. Because of the complexity of computer vision, the hidden layer(s) can have large numbers of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are doing a binary classification problem, we only need 1 neuron in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not recommended to use ReLU as the activation function of the output layer. Instead, the Sigmoid function would work better in this binary classification problem. If we where doing a multi-class classification problem, then Softmax would be the choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By compiling, we mean connecting the Neural Network to an optimizer, loss function and some metrics. We are using an **Atom Optimizer** to perform **Stochastic Gradient Descent**. Our **Loss Function** is the **Binary Cross Entropy Loss**. We also using **Accuracy Metrics** as this is the most relevant way to measure a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our training set, we train the neural network and use the test set to evaluate its performance. For this example we will run 30 **epochs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "251/251 [==============================] - 63s 249ms/step - loss: 0.6928 - accuracy: 0.5365 - val_loss: 0.6848 - val_accuracy: 0.5699\n",
      "Epoch 2/30\n",
      "251/251 [==============================] - 77s 308ms/step - loss: 0.6593 - accuracy: 0.6131 - val_loss: 0.6369 - val_accuracy: 0.6480\n",
      "Epoch 3/30\n",
      "251/251 [==============================] - 53s 212ms/step - loss: 0.6295 - accuracy: 0.6483 - val_loss: 0.5733 - val_accuracy: 0.6945\n",
      "Epoch 4/30\n",
      "251/251 [==============================] - 57s 229ms/step - loss: 0.6125 - accuracy: 0.6596 - val_loss: 0.5889 - val_accuracy: 0.6757\n",
      "Epoch 5/30\n",
      "251/251 [==============================] - 59s 235ms/step - loss: 0.5894 - accuracy: 0.6832 - val_loss: 0.5430 - val_accuracy: 0.7281\n",
      "Epoch 6/30\n",
      "251/251 [==============================] - 65s 259ms/step - loss: 0.5781 - accuracy: 0.6969 - val_loss: 0.5617 - val_accuracy: 0.7242\n",
      "Epoch 7/30\n",
      "251/251 [==============================] - 68s 271ms/step - loss: 0.5749 - accuracy: 0.6964 - val_loss: 0.5272 - val_accuracy: 0.7341\n",
      "Epoch 8/30\n",
      "251/251 [==============================] - 60s 239ms/step - loss: 0.5636 - accuracy: 0.7102 - val_loss: 0.5478 - val_accuracy: 0.7291\n",
      "Epoch 9/30\n",
      "251/251 [==============================] - 62s 249ms/step - loss: 0.5528 - accuracy: 0.7127 - val_loss: 0.5126 - val_accuracy: 0.7444\n",
      "Epoch 10/30\n",
      "251/251 [==============================] - 52s 206ms/step - loss: 0.5403 - accuracy: 0.7263 - val_loss: 0.4879 - val_accuracy: 0.7593\n",
      "Epoch 11/30\n",
      "251/251 [==============================] - 52s 206ms/step - loss: 0.5336 - accuracy: 0.7308 - val_loss: 0.4820 - val_accuracy: 0.7632\n",
      "Epoch 12/30\n",
      "251/251 [==============================] - 51s 205ms/step - loss: 0.5215 - accuracy: 0.7365 - val_loss: 0.5091 - val_accuracy: 0.7558\n",
      "Epoch 13/30\n",
      "251/251 [==============================] - 52s 206ms/step - loss: 0.5207 - accuracy: 0.7429 - val_loss: 0.4721 - val_accuracy: 0.7785\n",
      "Epoch 14/30\n",
      "251/251 [==============================] - 52s 206ms/step - loss: 0.5239 - accuracy: 0.7422 - val_loss: 0.4773 - val_accuracy: 0.7781\n",
      "Epoch 15/30\n",
      "251/251 [==============================] - 52s 207ms/step - loss: 0.5143 - accuracy: 0.7423 - val_loss: 0.4623 - val_accuracy: 0.7879\n",
      "Epoch 16/30\n",
      "251/251 [==============================] - 51s 205ms/step - loss: 0.5048 - accuracy: 0.7488 - val_loss: 0.4752 - val_accuracy: 0.7662\n",
      "Epoch 17/30\n",
      "251/251 [==============================] - 51s 203ms/step - loss: 0.5061 - accuracy: 0.7499 - val_loss: 0.4753 - val_accuracy: 0.7716\n",
      "Epoch 18/30\n",
      "251/251 [==============================] - 51s 204ms/step - loss: 0.5005 - accuracy: 0.7489 - val_loss: 0.4686 - val_accuracy: 0.7790\n",
      "Epoch 19/30\n",
      "251/251 [==============================] - 52s 208ms/step - loss: 0.4924 - accuracy: 0.7594 - val_loss: 0.4604 - val_accuracy: 0.7766\n",
      "Epoch 20/30\n",
      "251/251 [==============================] - 52s 207ms/step - loss: 0.4965 - accuracy: 0.7589 - val_loss: 0.4488 - val_accuracy: 0.7909\n",
      "Epoch 21/30\n",
      "251/251 [==============================] - 73s 290ms/step - loss: 0.4979 - accuracy: 0.7555 - val_loss: 0.4401 - val_accuracy: 0.7949\n",
      "Epoch 22/30\n",
      "251/251 [==============================] - 65s 259ms/step - loss: 0.4922 - accuracy: 0.7609 - val_loss: 0.4634 - val_accuracy: 0.7879\n",
      "Epoch 23/30\n",
      "251/251 [==============================] - 53s 211ms/step - loss: 0.4869 - accuracy: 0.7625 - val_loss: 0.4474 - val_accuracy: 0.8028\n",
      "Epoch 24/30\n",
      "251/251 [==============================] - 59s 235ms/step - loss: 0.4904 - accuracy: 0.7638 - val_loss: 0.4699 - val_accuracy: 0.7776\n",
      "Epoch 25/30\n",
      "251/251 [==============================] - 52s 208ms/step - loss: 0.4800 - accuracy: 0.7686 - val_loss: 0.4516 - val_accuracy: 0.8003\n",
      "Epoch 26/30\n",
      "251/251 [==============================] - 53s 209ms/step - loss: 0.4752 - accuracy: 0.7764 - val_loss: 0.4580 - val_accuracy: 0.7919\n",
      "Epoch 27/30\n",
      "251/251 [==============================] - 71s 283ms/step - loss: 0.4757 - accuracy: 0.7724 - val_loss: 0.4418 - val_accuracy: 0.8043\n",
      "Epoch 28/30\n",
      "251/251 [==============================] - 52s 208ms/step - loss: 0.4825 - accuracy: 0.7626 - val_loss: 0.4584 - val_accuracy: 0.7865\n",
      "Epoch 29/30\n",
      "251/251 [==============================] - 77s 305ms/step - loss: 0.4731 - accuracy: 0.7716 - val_loss: 0.4451 - val_accuracy: 0.8057\n",
      "Epoch 30/30\n",
      "251/251 [==============================] - 69s 276ms/step - loss: 0.4628 - accuracy: 0.7791 - val_loss: 0.4825 - val_accuracy: 0.7711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x158ca18db90>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a prediction. We have an example image that we are going to check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align='center'>\n",
    "<img src='./assets/images/cnn-dog-test.jpg'>\n",
    "</p>\n",
    "\n",
    "<p align='center'>An image of a dog</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the image. The image MUST be the same size as the ones used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.load_img('./assets/images/cnn-dog-test.jpg', target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test image must also be converted into a 2D-array as this is the input that is expected by our Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method has to be called using the exact same format as was used during the training. Our Convolutional Neural Network was not trained on a single image but rather batches of images. So now we have an extra dimension of the batch. To solve this, we put our image in the batch so that the `predict` method can recognize the batch as the extra dimension. The batch is always the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "result = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model would either give us a 0 or a 1. Let us see what each one represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 0 represents cats and 1 represents dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the result of our prediction. Recall that our result would be an batch (of which there is only 1) of only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if result[0][0] == 1:\n",
    "    prediction = \"dog\"\n",
    "else:\n",
    "    prediction = \"cat\"\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align='center'>\n",
    "<img src='./assets/images/cnn-cat-test.jpg'>\n",
    "</p>\n",
    "\n",
    "<p align='center'>Image of a Cat</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = image.load_img('./assets/images/cnn-cat-test.jpg', target_size=(150, 150))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "if result[0][0] == 1:\n",
    "    prediction = \"dog\"\n",
    "else:\n",
    "    prediction = \"cat\"\n",
    "\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
