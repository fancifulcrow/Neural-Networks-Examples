{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following is a Convolutional Neural Network that differentiates between cats and dogs. The dataset used is from [Kaggle](https://www.kaggle.com/datasets/tongpython/cat-and-dog) and contains over 10k images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Training Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to perform some **transformations** (such as rotation, flip, zoom, translation, shear e.t.c) on the images of the training set. The idea behind these transformations is to expose the model to a diverse range of input variations during training, which helps prevent it from memorizing specific details of the training set. This way we can avoid **overfitting**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `rescale` parameter of the `ImageDataGenerator` will apply **feature scaling** to every pixel. By scaling the pixel values by 1/255, we normalize them to the range [0, 1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is a generator that will read images found in subfolers of 'data/train', and indefinitely generate batches of augmented image data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8005 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "training_set = train_datagen.flow_from_directory(\n",
        "        'data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        class_mode='binary') # this is a binary classification problem i.e either cats or dogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do not want to perform transformations on the validation data to avoid **data leakage**. We want to ensure that the evaluation of the model's performance reflects its ability to generalize to new, unseen data in a real-world scenario. We do however still need to normalize the data for our neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similary to the training, we create a generator for the validation set. Only difference is the directory of the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2023 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "val_set = val_datagen.flow_from_directory(\n",
        "        'data/validation',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Convolutional Neural Network is a sequence of layers. Therefore we are going to intialize our model with the `Sequential` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convolution Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add the Convolution Layer with the `Conv2D` class to our model while specifying `filters` (number of feature detectors you want to apply to your images), `kernel_size` (the size of the feature detectors), `activation` (the activation function) and `input_shape` (the input shape).\n",
        "\n",
        "When we add the very first layer, whether a convolution layer or a dense layer, we have to specify the input shape of our inputs. The input shape is (150, 150, 3) as the size of our images after preprocessing is 150x150 and we are using coloured images, 3 dimensions corresponding to the RGB channels.\n",
        "\n",
        "For other layers after the input layer, we do not specify the input shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(150, 150, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pooling Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we add a layer for pooling. More specifically, **max pooling**. The `MaxPool2D` class has two necessary parameters: `pool_size` (size of the pooling window) and `strides` (step size of the pooling window)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding a Second Convolution Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a second convolutional layer in a Convolutional Neural Network allows the model to capture higher-level features by learning more complex patterns and representations from the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flattening"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We follow up by flattening the results of our convolutions and poolings into a one-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can pass it on to a fully connected Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Join up to fully connected layers. Because of the complexity of computer vision, the hidden layer(s) can have large numbers of neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because we are doing a binary classification problem, we only need 1 neuron in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is not recommended to use ReLU as the activation function of the output layer. Instead, the Sigmoid function would work better in this binary classification problem. If we where doing a multi-class classification problem, then Softmax would be the choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compiling the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By compiling, we mean connecting the Neural Network to an optimizer, loss function and some metrics. We are using an **Atom Optimizer** to perform **Stochastic Gradient Descent**. Our **Loss Function** is the **Binary Cross Entropy Loss**. We also using **Accuracy Metrics** as this is the most relevant way to measure a classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our training set, we train the neural network and use the test set to evaluate its performance. For this example we will run 30 **epochs**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "251/251 [==============================] - 55s 216ms/step - loss: 0.7059 - accuracy: 0.5635 - val_loss: 0.6703 - val_accuracy: 0.5769\n",
            "Epoch 2/30\n",
            "251/251 [==============================] - 52s 207ms/step - loss: 0.6598 - accuracy: 0.6125 - val_loss: 0.6156 - val_accuracy: 0.6683\n",
            "Epoch 3/30\n",
            "251/251 [==============================] - 53s 211ms/step - loss: 0.6335 - accuracy: 0.6356 - val_loss: 0.6158 - val_accuracy: 0.6619\n",
            "Epoch 4/30\n",
            "251/251 [==============================] - 52s 208ms/step - loss: 0.6223 - accuracy: 0.6548 - val_loss: 0.5867 - val_accuracy: 0.6896\n",
            "Epoch 5/30\n",
            "251/251 [==============================] - 52s 208ms/step - loss: 0.6026 - accuracy: 0.6785 - val_loss: 0.5528 - val_accuracy: 0.7182\n",
            "Epoch 6/30\n",
            "251/251 [==============================] - 52s 206ms/step - loss: 0.5933 - accuracy: 0.6814 - val_loss: 0.5799 - val_accuracy: 0.6851\n",
            "Epoch 7/30\n",
            "251/251 [==============================] - 51s 202ms/step - loss: 0.5658 - accuracy: 0.7026 - val_loss: 0.5323 - val_accuracy: 0.7365\n",
            "Epoch 8/30\n",
            "251/251 [==============================] - 51s 202ms/step - loss: 0.5597 - accuracy: 0.7166 - val_loss: 0.5303 - val_accuracy: 0.7439\n",
            "Epoch 9/30\n",
            "251/251 [==============================] - 50s 199ms/step - loss: 0.5530 - accuracy: 0.7171 - val_loss: 0.5184 - val_accuracy: 0.7479\n",
            "Epoch 10/30\n",
            "251/251 [==============================] - 50s 201ms/step - loss: 0.5374 - accuracy: 0.7273 - val_loss: 0.5348 - val_accuracy: 0.7326\n",
            "Epoch 11/30\n",
            "251/251 [==============================] - 50s 201ms/step - loss: 0.5346 - accuracy: 0.7324 - val_loss: 0.4980 - val_accuracy: 0.7637\n",
            "Epoch 12/30\n",
            "251/251 [==============================] - 50s 199ms/step - loss: 0.5305 - accuracy: 0.7392 - val_loss: 0.5246 - val_accuracy: 0.7346\n",
            "Epoch 13/30\n",
            "251/251 [==============================] - 50s 200ms/step - loss: 0.5317 - accuracy: 0.7297 - val_loss: 0.4976 - val_accuracy: 0.7622\n",
            "Epoch 14/30\n",
            "251/251 [==============================] - 50s 199ms/step - loss: 0.5130 - accuracy: 0.7420 - val_loss: 0.5065 - val_accuracy: 0.7548\n",
            "Epoch 15/30\n",
            "251/251 [==============================] - 50s 200ms/step - loss: 0.5217 - accuracy: 0.7395 - val_loss: 0.4773 - val_accuracy: 0.7721\n",
            "Epoch 16/30\n",
            "251/251 [==============================] - 51s 202ms/step - loss: 0.5207 - accuracy: 0.7452 - val_loss: 0.4708 - val_accuracy: 0.7726\n",
            "Epoch 17/30\n",
            "251/251 [==============================] - 74s 294ms/step - loss: 0.5075 - accuracy: 0.7444 - val_loss: 0.5160 - val_accuracy: 0.7509\n",
            "Epoch 18/30\n",
            "251/251 [==============================] - 103s 410ms/step - loss: 0.5085 - accuracy: 0.7487 - val_loss: 0.4826 - val_accuracy: 0.7692\n",
            "Epoch 19/30\n",
            "251/251 [==============================] - 82s 327ms/step - loss: 0.5030 - accuracy: 0.7497 - val_loss: 0.4760 - val_accuracy: 0.7696\n",
            "Epoch 20/30\n",
            "251/251 [==============================] - 89s 355ms/step - loss: 0.5015 - accuracy: 0.7538 - val_loss: 0.4516 - val_accuracy: 0.7909\n",
            "Epoch 21/30\n",
            "251/251 [==============================] - 93s 369ms/step - loss: 0.4938 - accuracy: 0.7565 - val_loss: 0.4689 - val_accuracy: 0.7820\n",
            "Epoch 22/30\n",
            "251/251 [==============================] - 99s 394ms/step - loss: 0.4886 - accuracy: 0.7606 - val_loss: 0.4896 - val_accuracy: 0.7612\n",
            "Epoch 23/30\n",
            "251/251 [==============================] - 111s 441ms/step - loss: 0.4833 - accuracy: 0.7654 - val_loss: 0.5116 - val_accuracy: 0.7454\n",
            "Epoch 24/30\n",
            "251/251 [==============================] - 108s 430ms/step - loss: 0.4919 - accuracy: 0.7590 - val_loss: 0.4513 - val_accuracy: 0.7958\n",
            "Epoch 25/30\n",
            "251/251 [==============================] - 92s 369ms/step - loss: 0.4890 - accuracy: 0.7614 - val_loss: 0.4633 - val_accuracy: 0.7860\n",
            "Epoch 26/30\n",
            "251/251 [==============================] - 88s 351ms/step - loss: 0.4822 - accuracy: 0.7664 - val_loss: 0.4556 - val_accuracy: 0.7865\n",
            "Epoch 27/30\n",
            "251/251 [==============================] - 74s 296ms/step - loss: 0.4813 - accuracy: 0.7610 - val_loss: 0.4529 - val_accuracy: 0.7889\n",
            "Epoch 28/30\n",
            "251/251 [==============================] - 64s 257ms/step - loss: 0.4707 - accuracy: 0.7723 - val_loss: 0.4640 - val_accuracy: 0.7790\n",
            "Epoch 29/30\n",
            "251/251 [==============================] - 65s 259ms/step - loss: 0.4758 - accuracy: 0.7791 - val_loss: 0.4439 - val_accuracy: 0.7944\n",
            "Epoch 30/30\n",
            "251/251 [==============================] - 65s 260ms/step - loss: 0.4705 - accuracy: 0.7748 - val_loss: 0.4727 - val_accuracy: 0.7731\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x139573e4a50>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.fit(x=training_set, validation_data=val_set, epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time to make a prediction. We have an example image that we are going to check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align='center'>\n",
        "<img src='./assets/images/cnn-dog-test-1.jpg'>\n",
        "</p>\n",
        "\n",
        "<p align='center'>An image of a dog</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly, we load the image. The image MUST be the same size as the ones used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_image = image.load_img('./assets/images/cnn-dog-test-1.jpg', target_size=(150, 150))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our test image must also be converted into a 2D-array as this is the input that is expected by our Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_image = image.img_to_array(test_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In CNNs, the input usually expects a batch of images, even if you're predicting on just one image. The extra dimension represents the batch size. The shape of the input expected by the CNN should be in the format (batch_size, height, width, channels) So we must add the batch_size as the first dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_image = np.expand_dims(test_image, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we normalize the the test image and predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 202ms/step\n"
          ]
        }
      ],
      "source": [
        "result = cnn.predict(test_image / 255.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model would give us a value in the range [0, 1]. Let us see what each end represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'cats': 0, 'dogs': 1}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So a value of 0 would mean that the model is 100% certain it is a cat and a value of 1 would be that the model is 100% certain it is a dog."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us see the result of our prediction. Recall that our result would be a single batch of only one element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'dog'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if result[0][0] > 0.5:\n",
        "    prediction = \"dog\"\n",
        "else:\n",
        "    prediction = \"cat\"\n",
        "\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align='center'>\n",
        "<img src='./assets/images/cnn-cat-test-1.jpg'>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cat'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_image = image.load_img('./assets/images/cnn-cat-test-1.jpg', target_size=(150, 150))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "result = cnn.predict(test_image / 255.0)\n",
        "if result[0][0] > 0.5:\n",
        "    prediction = \"dog\"\n",
        "else:\n",
        "    prediction = \"cat\"\n",
        "\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align='center'>\n",
        "<img src='./assets/images/cnn-dog-test-2.jpg'>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'dog'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_image = image.load_img('./assets/images/cnn-dog-test-2.jpg', target_size=(150, 150))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "result = cnn.predict(test_image / 255.0)\n",
        "if result[0][0] > 0.5:\n",
        "    prediction = \"dog\"\n",
        "else:\n",
        "    prediction = \"cat\"\n",
        "\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align='center'>\n",
        "<img src='./assets/images/cnn-cat-test-2.jpg'>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cat'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_image = image.load_img('./assets/images/cnn-cat-test-2.jpg', target_size=(150, 150))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "result = cnn.predict(test_image / 255.0)\n",
        "if result[0][0] > 0.5:\n",
        "    prediction = \"dog\"\n",
        "else:\n",
        "    prediction = \"cat\"\n",
        "\n",
        "prediction"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
